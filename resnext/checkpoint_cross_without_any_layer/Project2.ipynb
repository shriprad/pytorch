{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Project2_Kaggle_Competition_Classification_soln.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CDdzARUfzQo",
        "colab_type": "text"
      },
      "source": [
        "# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
        "\n",
        "#### Maximum Points: 100\n",
        "\n",
        "<div>\n",
        "    <table>\n",
        "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
        "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
        "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
        "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
        "    </table>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0cd_JHifzQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from datetime import timedelta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW7_Le_YfzQv",
        "colab_type": "text"
      },
      "source": [
        "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
        "\n",
        "In this section, you have to write a class or methods that will be used to get training and validation data loader.\n",
        "\n",
        "For example,\n",
        "\n",
        "```\n",
        "def get_data(args1, *agrs):\n",
        "    ....\n",
        "    ....\n",
        "    return train_loader, test_loader\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KwQwkhGfzQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KenyanFood13Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    This custom dataset class take root directory and train flag, \n",
        "    and return dataset training dataset id train flag is true \n",
        "    else is return validation dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_root, data_set='train', folds=5, fold=0, transform=None):\n",
        "        \n",
        "        \"\"\"\n",
        "        init method of the class.\n",
        "        \n",
        "         Parameters:\n",
        "         data_root (string): path of root directory.\n",
        "         data_set (string): 'train' | 'val' | 'test'\n",
        "         folds (int): number of Folds\n",
        "         fold (int): part of trainingdata for validation (StratifiedKFold)                        \n",
        "         transform (method): method that will take PIL image and transforms it.\n",
        "         \n",
        "        \"\"\"\n",
        "        \n",
        "        # set transform attribute\n",
        "        self.transform = transform    #Transforms passed to initalizer\n",
        "        \n",
        "        # training data path, this will be used as data root\n",
        "        self.img_dir = os.path.join(data_root, 'images', 'images')\n",
        "        \n",
        "        # Training Data\n",
        "        train_csv_path = os.path.join(data_root, 'train.csv')       ##loads the images\n",
        "        train_data = pd.read_csv(train_csv_path, delimiter=',')\n",
        "        \n",
        "        # Get all Classes\n",
        "        self.classes = train_data['class'].unique()\n",
        "        self.classes.sort()\n",
        "        \n",
        "        # number of classes \n",
        "        self.num_classes = len(self.classes)\n",
        "        \n",
        "        # Label -> Id\n",
        "        self._label_to_id = {}\n",
        "        \n",
        "        # Id -> Label\n",
        "        self._id_to_label = {}\n",
        "        \n",
        "        for i, label in enumerate(self.classes):\n",
        "            self._label_to_id[label] = i\n",
        "            self._id_to_label[i] = label\n",
        "        \n",
        "        # map class names to clas ids\n",
        "        train_data['class_id'] = train_data['class'].map(self._label_to_id)\n",
        "        \n",
        "        \n",
        "        if data_set == 'test':\n",
        "            self.train = False\n",
        "            \n",
        "            test_csv_path = os.path.join(data_root, 'test.csv')\n",
        "            self.data_df= pd.read_csv(test_csv_path, delimiter=',')\n",
        "             \n",
        "        else:\n",
        "            self.train = True\n",
        "            \n",
        "            # Use a stratified Split for validation during training\n",
        "            cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "            split = list(cv.split(train_data, train_data['class_id']))\n",
        "\n",
        "            self.train_idx, self.val_idx = split[fold]\n",
        "        \n",
        "            train_df = train_data.iloc[self.train_idx]\n",
        "            val_df = train_data.iloc[self.val_idx]\n",
        "            \n",
        "           \n",
        "            # self.counter = collections.Counter(train_df['class_id'].values)\n",
        "            # self.class_count = np.zeros(self.num_classes)\n",
        "            # for i in range(self.num_classes):\n",
        "            #     self.class_count[i] = self.counter[i]\n",
        "                \n",
        "            # self.class_weights = [1 - (x / np.sum(self.class_count)) for x in self.class_count] \n",
        "            \n",
        "            self.class_weights = compute_class_weight('balanced',\n",
        "                                                      np.unique(train_df['class_id'].values),\n",
        "                                                      train_df['class_id'].values)\n",
        "            \n",
        "            if data_set == 'train':\n",
        "                self.data_df = train_df\n",
        "            else:\n",
        "                self.data_df = val_df\n",
        "            \n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        return length of the dataset\n",
        "        \"\"\"\n",
        "        return self.data_df.shape[0]\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        For given index, return images and preprocessing.\n",
        "        \"\"\"\n",
        "        \n",
        "        img_path = os.path.join(self.img_dir, str(self.data_df['id'].iloc[idx]) + '.jpg')\n",
        "        \n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        if self.train:  \n",
        "            target = self.data_df['class_id'].iloc[idx]\n",
        "            return img, target  \n",
        "\n",
        "        else:\n",
        "            image_id = self.data_df['id'].iloc[idx]\n",
        "            return img\n",
        "                \n",
        "        \n",
        "    def label_to_id(self, label):\n",
        "        \"\"\"\n",
        "        latin name mapping to class label id\n",
        "        \"\"\"\n",
        "        return self._label_to_id[label]\n",
        "    \n",
        "    def id_to_label(self, id):\n",
        "        \"\"\"\n",
        "        class label to latin name mapping\n",
        "        \"\"\"\n",
        "        return self._id_to_label[id]\n",
        "    \n",
        "    def get_split(self):\n",
        "        \"\"\"\n",
        "        get the split ids\n",
        "        \"\"\"\n",
        "        return self.train_idx, self.val_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_A-IBvpfzQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_transforms():\n",
        "    \n",
        "    # mean and std of imagenet dataset\n",
        "    mean = [0.485, 0.456, 0.406] \n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    tf = transforms.Compose([transforms.Resize(320),\n",
        "                             transforms.CenterCrop(288),\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize(mean, std)\n",
        "                             ])\n",
        "    return tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhcnUfrwfzQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_transforms():\n",
        "    \n",
        "    # mean and std of imagenet dataset\n",
        "    mean = [0.485, 0.456, 0.406] \n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    \n",
        "    tf = transforms.Compose([transforms.Resize(320),\n",
        "                             transforms.ColorJitter(brightness=0.2,\n",
        "                                                    saturation=0.2,\n",
        "                                                    contrast=0.2),\n",
        "                             transforms.RandomAffine(degrees=0.45,\n",
        "                                                      translate=(0.2, 0.2),\n",
        "                                                      scale=(0.8, 1.2)),\n",
        "                             transforms.RandomHorizontalFlip(0.5),\n",
        "                             transforms.RandomVerticalFlip(0.5),\n",
        "                             transforms.RandomCrop(288),\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize(mean, std),\n",
        "                             ])                                \n",
        "    return tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOmOGYZffzQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(batch_size, data_root, num_workers=4, folds=5, fold=0):\n",
        "    \n",
        "    # train\n",
        "    train_dataset =  KenyanFood13Dataset(data_root, data_set='train',\n",
        "                                         folds=folds,\n",
        "                                         fold=fold,\n",
        "                                         transform=training_transforms())\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=num_workers)\n",
        "    # validation\n",
        "    val_dataset = KenyanFood13Dataset(data_root,\n",
        "                                      data_set='val',\n",
        "                                      folds=folds,\n",
        "                                      fold=fold,\n",
        "                                      transform=validation_transforms())\n",
        "    \n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             num_workers=num_workers)\n",
        "    \n",
        "    # test\n",
        "    test_dataset = KenyanFood13Dataset(data_root,\n",
        "                                       data_set='test',\n",
        "                                       transform=validation_transforms())\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              num_workers=num_workers)\n",
        "    \n",
        "    \n",
        "    return train_loader, val_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs9syVFtfzRE",
        "colab_type": "text"
      },
      "source": [
        "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
        "\n",
        "Define your configuration in this section.\n",
        "\n",
        "For example,\n",
        "\n",
        "```\n",
        "@dataclass\n",
        "class TrainingConfiguration:\n",
        "    '''\n",
        "    Describes configuration of the training process\n",
        "    '''\n",
        "    batch_size: int = 10 \n",
        "    epochs_count: int = 50  \n",
        "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
        "    log_interval: int = 5  \n",
        "    test_interval: int = 1  \n",
        "    data_root: str = \"./cat-dog-panda\" \n",
        "    num_workers: int = 2  \n",
        "    device: str = 'cuda'  \n",
        "    \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5_o5XKUfzRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@dataclass\n",
        "class SystemConfiguration:\n",
        "    '''\n",
        "    Describes the common system setting needed for reproducible training\n",
        "    '''\n",
        "    seed: int = 21  # seed number to set the state of all random number generators\n",
        "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
        "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAaHqqLLfzRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@dataclass\n",
        "class TrainingConfiguration:\n",
        "    '''\n",
        "    Describes configuration of the training process\n",
        "    '''\n",
        "    batch_size: int = 15\n",
        "    epochs_count: int = 20 \n",
        "    init_learning_rate: float = 0.0005\n",
        "    decay_rate: float = 0.2  \n",
        "    log_interval: int = 500  \n",
        "    test_interval: int = 1  \n",
        "    data_root: str = \"./\" \n",
        "    folds: int = 5   # split dataset in number of folds\n",
        "    fold: int = 0    # current fold for training\n",
        "    num_workers: int = 2 \n",
        " #   model: str = 'resnext50_32x4d'\n",
        "    model: str = 'inception_v3'\n",
        "\n",
        "    \n",
        "#    save_name: str = \"resnext50_32x4d.pt\"\n",
        "    save_name: str = \"inception_v31.pt\"\n",
        "\n",
        "    device: str = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDNbZalGfzRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_system(system_config: SystemConfiguration) -> None:\n",
        "    torch.manual_seed(system_config.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
        "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEQC5SaNfzRT",
        "colab_type": "text"
      },
      "source": [
        "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
        "\n",
        "Define methods or classes that will be used in model evaluation, for example, accuracy, f1-score, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLtk7SbtfzRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_torch(y_true:torch.Tensor, y_pred:torch.Tensor) -> torch.Tensor:\n",
        "    \n",
        "    '''Calculate Accuracy.'''\n",
        "    \n",
        "    assert y_true.ndim == 1\n",
        "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
        "    \n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "    \n",
        "    accuracy = y_pred.eq(y_true).float().mean()\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOR0eYv6fzRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_numpy(y_true, y_pred):\n",
        "    \n",
        "    '''Calculate Accuracy.'''\n",
        "    \n",
        "    assert y_true.ndim == 1\n",
        "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
        "    \n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "    \n",
        "    accuracy = np.equal(y_pred, y_true).astype(np.int).mean()\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXKFdOILfzRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_matrix(y_true, y_pred):\n",
        "    \n",
        "    '''Calculate Confusion Matrix'''\n",
        "    \n",
        "    assert y_true.ndim == 1\n",
        "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
        "    \n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "    \n",
        "    return metrics.confusion_matrix(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF2LYTD1fzRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classification_report(y_true, y_pred, target_names=None):\n",
        "    \n",
        "    '''Classification Report:\n",
        "       per class:  precision\n",
        "                   recall\n",
        "                   f-score\n",
        "    '''\n",
        "    \n",
        "    assert y_true.ndim == 1\n",
        "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
        "    \n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "    \n",
        "    report = metrics.classification_report(y_true,\n",
        "                                           y_pred,\n",
        "                                           target_names=target_names,\n",
        "                                           digits=3,\n",
        "                                           zero_division=0)\n",
        "    \n",
        "    return report\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f0yKJCSfzRo",
        "colab_type": "text"
      },
      "source": [
        "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
        "\n",
        "Write the methods or classes that will be used for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUiS9-MVfzRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "          train_loader: torch.utils.data.DataLoader, epoch_idx: int, tb_writer: SummaryWriter) -> None:\n",
        "    \n",
        "    # change model in training mood\n",
        "    model.train()\n",
        "    \n",
        "    # to get batch loss\n",
        "    batch_loss = np.array([])\n",
        "    \n",
        "    # to get batch accuracy\n",
        "    batch_acc = np.array([])\n",
        "    \n",
        "    # class weights\n",
        "    class_weights = train_loader.dataset.class_weights\n",
        "    #print(\"Using Class Weights:\")\n",
        "    #for i, weight in enumerate(class_weights):\n",
        "    #    print(\"{} : {}\".format(i, weight))\n",
        "    class_weights = torch.FloatTensor(class_weights).to(TrainingConfiguration.device)\n",
        "    weighted_cross_entropy = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        # clone target\n",
        "        indx_target = target.clone()\n",
        "        # send data to device (its is mandatory if GPU has to be used)\n",
        "        data = data.to(train_config.device)\n",
        "        # send target to device\n",
        "        target = target.to(train_config.device)\n",
        "\n",
        "        # reset parameters gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass to the model\n",
        "        output = model(data)\n",
        "        \n",
        "        # cross entropy loss\n",
        "        loss = weighted_cross_entropy(output, target)\n",
        "        #loss = F.cross_entropy(output, target)\n",
        "        \n",
        "        # find gradients w.r.t training parameters\n",
        "        loss.backward()\n",
        "        # Update parameters using gardients\n",
        "        optimizer.step()\n",
        "        \n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\n",
        "        \n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "            \n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]  \n",
        "                        \n",
        "        # correct prediction\n",
        "        correct = pred.cpu().eq(indx_target).sum()\n",
        "            \n",
        "        # accuracy\n",
        "        acc = float(correct) / float(len(data))\n",
        "        \n",
        "        batch_acc = np.append(batch_acc, [acc])\n",
        "\n",
        "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:\n",
        "            \n",
        "            total_batch = epoch_idx * len(train_loader.dataset)/train_config.batch_size + batch_idx\n",
        "            tb_writer.add_scalar('Loss/train-batch', loss.item(), total_batch)\n",
        "            tb_writer.add_scalar('Accuracy/train-batch', acc, total_batch)\n",
        "            \n",
        "    epoch_loss = batch_loss.mean()\n",
        "    epoch_acc = batch_acc.mean()\n",
        "    print('Train Loss: {:.6f} Accuracy: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "    return epoch_loss, epoch_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hLslqqPfzRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_batch(model, device, batch_input):\n",
        "    \"\"\"\n",
        "    get prediction for batch inputs\n",
        "    \"\"\"\n",
        "    \n",
        "    # send model to cpu/cuda according to your system configuration\n",
        "    model.to(device)\n",
        "    \n",
        "    # it is important to do model.eval() before prediction\n",
        "    model.eval()\n",
        "    \n",
        "    data, target = batch_input\n",
        "\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    output = model(data)\n",
        "    \n",
        "    # Loss\n",
        "    loss = F.cross_entropy(output, target).item()\n",
        "    \n",
        "    # Score to probability using softmax\n",
        "    prob = F.softmax(output, dim=1)\n",
        "    \n",
        "    return loss, prob.data.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpwfWeHsfzRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_dataset(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    get targets and prediction probabilities\n",
        "    \"\"\"\n",
        "    \n",
        "    losses = []\n",
        "    probs = []\n",
        "    targets = []\n",
        "    \n",
        "    for _, (data, target) in enumerate(dataloader):\n",
        "        \n",
        "        loss, prob = validate_batch(model, device, (data, target))\n",
        "        \n",
        "        losses.append(loss)\n",
        "        probs.append(prob)\n",
        "        targets.append(target.numpy())\n",
        "        \n",
        "\n",
        "        \n",
        "    targets = np.concatenate(targets)\n",
        "    targets = targets.astype(np.int)\n",
        "    \n",
        "    probs = np.concatenate(probs, axis=0)\n",
        "    \n",
        "    losses = np.array(loss)\n",
        "    \n",
        "    return losses, targets, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JosH34x4fzR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(train_config: TrainingConfiguration,\n",
        "             model: nn.Module,\n",
        "             dataloader: torch.utils.data.DataLoader) -> float:\n",
        "    \n",
        "    \n",
        "    losses, targets, probs = validate_dataset(model, dataloader, train_config.device)\n",
        "    \n",
        "    loss = losses.mean()\n",
        "    \n",
        "    predictions = probs.argmax(axis=1)\n",
        "    count_corect_predictions = np.equal(targets, predictions).astype(np.int).sum()\n",
        "    \n",
        "    # calculate metric\n",
        "    accuracy = accuracy_numpy(targets, predictions)\n",
        "    \n",
        "    print('Validation loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
        "          loss, count_corect_predictions, len(dataloader.dataset), accuracy * 100))\n",
        "    \n",
        "    report = classification_report(targets, predictions, target_names=dataloader.dataset.classes)\n",
        "    \n",
        "    return loss, accuracy, report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yutd8YaUfzR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(train_config: TrainingConfiguration,\n",
        "            model: nn.Module,\n",
        "            dataloader: torch.utils.data.DataLoader) -> float:\n",
        "    \n",
        "    probs = []\n",
        "    model.to(TrainingConfiguration.device)\n",
        "    model.eval()\n",
        "    \n",
        "    for data in dataloader:\n",
        "        \n",
        "        data = data.to(TrainingConfiguration.device)\n",
        "        \n",
        "        output = model(data)\n",
        "        \n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "        \n",
        "        probs.append(prob.cpu().detach().numpy())\n",
        "       \n",
        "    probs = np.concatenate(probs, axis=0)\n",
        "    \n",
        "    return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms2qIgmgfzR-",
        "colab_type": "text"
      },
      "source": [
        "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
        "\n",
        "Define your model in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTNSvXKZnZgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inception_v3(feature_extract= True, num_class=13):\n",
        "  model =models.inception_v3(pretrained=True,aux_logits=False)\n",
        "\n",
        "  if feature_extract:\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "\n",
        "    # for param in model.Mixed_6a.parameters():\n",
        "      \n",
        "    #   param.requires_grad=True\n",
        "\n",
        "\n",
        "\n",
        "    # for param in model.Mixed_6b.parameters():\n",
        "      \n",
        "    #   param.requires_grad=True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # for param in model.Mixed_6c.parameters():\n",
        "      \n",
        "    #   param.requires_grad=True\n",
        "\n",
        "\n",
        "    # for param in model.Mixed_6d.parameters():\n",
        "      \n",
        "    #   param.requires_grad=True\n",
        "\n",
        "\n",
        "\n",
        "    # for param in model.Mixed_6e.parameters():\n",
        "      \n",
        "    #   param.requires_grad=True\n",
        "\n",
        "\n",
        "    # for param in model.Mixed_7a.parameters():\n",
        "      \n",
        "    #   param.requires_grad=True\n",
        "\n",
        "    # for param in model.Mixed_7b.parameters():\n",
        "      \n",
        "    #   param.requires_grad=True\n",
        "\n",
        "\n",
        "    # for param in model.Mixed_7c.parameters():\n",
        "      \n",
        "    #   param.requires_grad=True\n",
        "      \n",
        "\n",
        "\n",
        "    # for param in model.parameters():\n",
        "    #   param.aux_logits = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    last_layer_in = model.fc.in_features\n",
        "\n",
        "    model.fc = nn.Linear(last_layer_in , num_class)\n",
        "\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO9uGm_pfzR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnext50_32x4d(transfer_learning=True, num_class=13):\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    \n",
        "    if transfer_learning:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # for param in model.layer3.parameters():\n",
        "          \n",
        "          \n",
        "        #   param.requires_grad = True\n",
        "            \n",
        "        # for param in model.layer4.parameters():\n",
        "        #     param.requires_grad = True\n",
        "                              \n",
        "    last_layer_in = model.fc.in_features\n",
        "    model.fc = nn.Linear(last_layer_in, num_class)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3GlA_g8fzSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet50(transfer_learning=True, num_class=13):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    \n",
        "    if transfer_learning:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        \n",
        "        for param in model.layer3.parameters():\n",
        "            param.requires_grad = True\n",
        "            \n",
        "        for param in model.layer4.parameters():\n",
        "            param.requires_grad = True\n",
        "                   \n",
        "    last_layer_in = model.fc.in_features\n",
        "    model.fc = nn.Linear(last_layer_in, num_class)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6KG_8IYc_eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_4g0rfdfzSG",
        "colab_type": "text"
      },
      "source": [
        "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
        "\n",
        "Define your methods or classes which are not covered in the above sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTA8YRUTfzSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, device, model_dir='models', model_file_name='classifier.pt'):\n",
        "    \n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    model_path = os.path.join(model_dir, model_file_name)\n",
        "\n",
        "    # make sure you transfer the model to cpu.\n",
        "    if device == 'cuda':\n",
        "        model.to('cpu')\n",
        "\n",
        "    # save the state_dict\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "    if device == 'cuda':\n",
        "        model.to('cuda')\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfiS9FcdfzSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model, model_dir='models', model_file_name='cat_dog_panda_classifier.pt'):\n",
        "    model_path = os.path.join(model_dir, model_file_name)\n",
        "\n",
        "    # loading the model and getting model parameters by using load_state_dict\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfX5hVcDfzSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_pr_curves_to_tensorboard(model, dataloader, device, tb_writer, epoch):\n",
        "    \"\"\"\n",
        "    Add precession and recall curve to tensorboard.\n",
        "    \"\"\"\n",
        "    \n",
        "    _, targets, pred_prob = validate_dataset(model, dataloader, device)\n",
        "    \n",
        "    for cls_idx in range(dataloader.dataset.num_classes):\n",
        "        binary_target = targets == cls_idx\n",
        "        true_prediction_prob = pred_prob[:, cls_idx]\n",
        "        \n",
        "        tb_writer.add_pr_curve(dataloader.dataset.id_to_label(cls_idx), \n",
        "                               binary_target, \n",
        "                               true_prediction_prob, \n",
        "                               global_step=epoch)\n",
        "        \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvFM8zgufzST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_model_weights_as_histogram(model, tb_writer, epoch):\n",
        "    for name, param in model.named_parameters():\n",
        "        tb_writer.add_histogram(name.replace('.', '/'), param.data.cpu().abs(), epoch)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2RMW0tEfzSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_network_graph_tensorboard(model, inputs, tb_writer):\n",
        "    tb_writer.add_graph(model, inputs)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVQqIpbOfzSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sec_to_min(seconds):\n",
        "    \n",
        "    seconds = int(seconds)\n",
        "    minutes = seconds // 60\n",
        "    seconds_remaining = seconds % 60\n",
        "    \n",
        "    if seconds_remaining < 10:\n",
        "        seconds_remaining = '0{}'.format(seconds_remaining)\n",
        "    \n",
        "    return '{}:{}'.format(minutes, seconds_remaining)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8I-8yp_fzSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sec_to_time(seconds):\n",
        "    return \"{:0>8}\".format(str(timedelta(seconds=int(seconds))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nA3t2jffzSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_seperator(length=80, character='-', name=None):\n",
        "    \n",
        "    if name:\n",
        "        rest = len(name) % 2\n",
        "        repeat = length // 2 - len(name) // 2 - 2\n",
        "        seperator_1 = character * repeat\n",
        "        \n",
        "        if rest:\n",
        "            seperator_2 = character * (repeat-1)\n",
        "        else:\n",
        "            seperator_2 = seperator_1\n",
        "    \n",
        "        print(\"{}[ {} ]{}\".format(seperator_1, name, seperator_2))\n",
        "         \n",
        "    else:\n",
        "        seperator = character * length\n",
        "        print(seperator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVozkteMfzSn",
        "colab_type": "text"
      },
      "source": [
        "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
        "\n",
        "Choose your optimizer and LR-scheduler and use the above methods and classes to train your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Fji8FFfzSo",
        "colab_type": "text"
      },
      "source": [
        "#### Select Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uzr4PPLfzSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# intialize config\n",
        "train_config=TrainingConfiguration()\n",
        "sys_config = SystemConfiguration()\n",
        "\n",
        "# set Model\n",
        "train_config.model = 'inception_v3'\n",
        "#train_config.model = 'resnet50'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baWTEcbQfzSs",
        "colab_type": "text"
      },
      "source": [
        "#### Optimizer and Scheduler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdWTOBCWfzSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimizer_and_scheduler(model):\n",
        "    train_config = TrainingConfiguration()\n",
        "\n",
        "    init_learning_rate = train_config.init_learning_rate\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr = init_learning_rate)\n",
        "\n",
        "    decay_rate = train_config.decay_rate\n",
        "\n",
        "    lmbda = lambda epoch: 1/(1 + decay_rate * epoch)\n",
        "\n",
        "    # Scheduler\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)\n",
        "    \n",
        "    return optimizer, scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Az61mYfzSx",
        "colab_type": "text"
      },
      "source": [
        "#### Main Function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbOMIOxifzSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(model, optimizer, tb_writer, scheduler=None, system_configuration=SystemConfiguration(), \n",
        "         training_configuration=TrainingConfiguration()):\n",
        "    \n",
        "    # system configuration\n",
        "    setup_system(system_configuration)\n",
        "\n",
        "    # batch size\n",
        "    batch_size_to_set = training_configuration.batch_size\n",
        "    \n",
        "    # num_workers\n",
        "    num_workers_to_set = training_configuration.num_workers\n",
        "    \n",
        "    # if GPU is available use training config, \n",
        "    # else lowers batch_size, num_workers and epochs count\n",
        "    if torch.cuda.is_available():\n",
        "        training_configuration.device = \"cuda\"\n",
        "    else:\n",
        "        training_configuration.device = \"cpu\"\n",
        "        training_configuration.batch_size_to_set = 16\n",
        "        training_configuration.num_workers_to_set = 2\n",
        "\n",
        "    # data loader\n",
        "    train_loader, test_loader, _ = get_data(batch_size=batch_size_to_set,\n",
        "                                            data_root=training_configuration.data_root,\n",
        "                                            num_workers=num_workers_to_set,\n",
        "                                            folds=training_configuration.folds,\n",
        "                                            fold=training_configuration.fold)\n",
        "      \n",
        "    # send model to device (GPU/CPU)\n",
        "    model.to(training_configuration.device)\n",
        "    \n",
        "    \n",
        "    # add network graph with inputs info\n",
        "    images, labels = next(iter(test_loader))\n",
        "    images = images.to(training_configuration.device)\n",
        "    add_network_graph_tensorboard(model, images, tb_writer)\n",
        "\n",
        "    best_loss = torch.tensor(np.inf)\n",
        "    best_accuracy = 0\n",
        "    best_report = None\n",
        "    best_epoch = 0\n",
        "    \n",
        "    # epoch train/test loss\n",
        "    epoch_train_loss = np.array([])\n",
        "    epoch_test_loss = np.array([])\n",
        "    \n",
        "    # epch train/test accuracy\n",
        "    epoch_train_acc = np.array([])\n",
        "    epoch_test_acc = np.array([])  \n",
        "    \n",
        "    # Start Training\n",
        "    print_seperator(80, '-', name='Training: {}'.format(training_configuration.model))\n",
        "    print(\"Batch Size: {}\".format(training_configuration.batch_size))\n",
        "    print(\"Epochs: {}\".format(training_configuration.epochs_count))\n",
        "    print(\"Fold: {} ({}/{})\".format(training_configuration.fold,\n",
        "                                    training_configuration.fold+1,\n",
        "                                    training_configuration.folds))\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Calculate Initial Test Loss\n",
        "    print_seperator(80, '-', name='Initial Fold: {}'.format(training_configuration.fold))\n",
        "    init_val_loss, init_val_accuracy, _ = validate(training_configuration, model, test_loader)\n",
        "    print(\"Initial Validation Loss : {:.6f}, \\nInitial Validation Accuracy : {:.3f}%\".format(init_val_loss, init_val_accuracy*100))\n",
        "    \n",
        "\n",
        "    # trainig time measurement\n",
        "    t_begin = time.time()\n",
        "    for epoch in range(training_configuration.epochs_count):\n",
        "        \n",
        "        print_seperator(80, '-', name='Epoch: {}'.format(epoch))\n",
        "        if scheduler is not None:\n",
        "            tb_writer.add_scalar('Learning_Rate/schedule', scheduler.get_last_lr()[0], epoch)\n",
        "        else:\n",
        "            tb_writer.add_scalar('Learning_Rate/schedule', training_configuration.init_learning_rate, epoch)    \n",
        "            \n",
        "        # Train\n",
        "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch, tb_writer)\n",
        "        \n",
        "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
        "        \n",
        "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
        "        \n",
        "        # add scalar (loss/accuracy) to tensorboard\n",
        "        tb_writer.add_scalar('Loss/Train',train_loss, epoch)\n",
        "        tb_writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
        "\n",
        "        elapsed_time = time.time() - t_begin\n",
        "        speed_epoch = elapsed_time / (epoch + 1)\n",
        "        speed_batch = speed_epoch / len(train_loader)\n",
        "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
        "        \n",
        "        print(\"Elapsed {}, {} time/epoch, {:.2f} s/batch, remaining {}\".format(\n",
        "                sec_to_time(elapsed_time), sec_to_time(speed_epoch), speed_batch, sec_to_time(eta)))\n",
        "        \n",
        "        # add time metadata to tensorboard\n",
        "        tb_writer.add_scalar('Time/min_training', elapsed_time/60, epoch)\n",
        "        tb_writer.add_scalar('Time/min_per_epoch', speed_epoch/60, epoch)\n",
        "        tb_writer.add_scalar('Time/sec_per_batch', speed_batch, epoch)\n",
        "        tb_writer.add_scalar('Time/min_remaining', eta/60, epoch)\n",
        "        \n",
        "\n",
        "        # Validate\n",
        "        if epoch % training_configuration.test_interval == 0:\n",
        "            current_loss, current_accuracy, report = validate(training_configuration, model, test_loader)\n",
        "            \n",
        "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
        "        \n",
        "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
        "            \n",
        "            \n",
        "            if current_accuracy > best_accuracy:\n",
        "                best_accuracy = current_accuracy\n",
        "                best_report = report\n",
        "                best_epoch = epoch\n",
        "                print('Model Improved. Saving the Model.')\n",
        "                save_model(model, \n",
        "                           device=training_configuration.device,\n",
        "                           model_file_name=training_configuration.save_name)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # add scalar (loss/accuracy) to tensorboard\n",
        "            tb_writer.add_scalar('Loss/Validation', current_loss, epoch)\n",
        "            tb_writer.add_scalar('Accuracy/Validation', current_accuracy, epoch)\n",
        "            \n",
        "            # add scalars (loss/accuracy) to tensorboard\n",
        "            #tb_writer.add_scalars('Loss/train-val', {'train': train_loss, \n",
        "            #                               'validation': current_loss}, epoch)\n",
        "            #tb_writer.add_scalars('Accuracy/train-val', {'train': train_acc, \n",
        "            #                                   'validation': current_accuracy}, epoch)\n",
        "            \n",
        "            if current_loss < best_loss:\n",
        "                best_loss = current_loss\n",
        "                        \n",
        "        # scheduler step/ update learning rate\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "              \n",
        "        # adding model weights to tensorboard as histogram\n",
        "        #add_model_weights_as_histogram(model, tb_writer, epoch)\n",
        "        \n",
        "        # add pr curves to tensor board\n",
        "        #add_pr_curves_to_tensorboard(model, test_loader, \n",
        "        #                             training_configuration.device, \n",
        "        #                             tb_writer, epoch)\n",
        "        \n",
        "    print_seperator(80, '-', name='Result Fold: {}'.format(training_configuration.fold))\n",
        "    print(\"Total time: {} - Accuracy: {:.3f}% - Best Epoch: {}\".format(sec_to_time(time.time()-t_begin),\n",
        "                                                                          best_accuracy * 100,\n",
        "                                                                          best_epoch))\n",
        "    print(best_report)\n",
        "    print_seperator(80, '-', name='End Fold: {}'.format(training_configuration.fold))\n",
        "    print()\n",
        "    \n",
        "    \n",
        "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YqlQ5HFfzS3",
        "colab_type": "text"
      },
      "source": [
        "#### Train Folds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2nGNrDHfzS4",
        "colab_type": "code",
        "outputId": "914d39cf-6a4d-4872-ed94-15ee7d84e6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# cllect results of all folds\n",
        "train_results_folds = {}\n",
        "\n",
        "# train folds for crossvalidation\n",
        "for fold in range(0, train_config.folds):\n",
        "    \n",
        "    if train_config.model == 'inception_v3':\n",
        "         #model = torch.hub.load('pytorch/vision:v0.5.0', 'inception_v3', pretrained=True,,num_class=13,aux_logits=False)\n",
        "\n",
        "         #model = resnext50_32x4d(transfer_learning=True, num_class=13)\n",
        "         model = inception_v3(feature_extract=True, num_class=13)\n",
        "    else:\n",
        "      \n",
        "        model = resnet50(transfer_learning=True, num_class=13)\n",
        "\n",
        "    # get optimizer and scheduler\n",
        "    optimizer, scheduler = get_optimizer_and_scheduler(model)\n",
        "        \n",
        "    # tensorboard summary writer\n",
        "    tb_writer = SummaryWriter('logs/fold_{}/{}'.format(fold, train_config.model))  \n",
        "    \n",
        "    # set training config current fold and save name\n",
        "    train_config.fold = fold\n",
        "    train_config.save_name = 'fold_{}_{}.pt'.format(fold, train_config.model)\n",
        "    \n",
        "    # train and validate\n",
        "    model, train_loss, train_acc, val_loss, val_acc = main(model, \n",
        "                                                           optimizer,\n",
        "                                                           tb_writer,\n",
        "                                                           scheduler,\n",
        "                                                           sys_config,\n",
        "                                                           training_configuration=train_config)\n",
        "    \n",
        "    # collect results of all folds of current fold\n",
        "    train_results_folds[fold] = {\"train_loss\": train_loss,\n",
        "                                 \"train_acc\": train_acc,\n",
        "                                 \"val_loss\": val_loss,\n",
        "                                 \"val_acc\": val_acc}\n",
        "    \n",
        "    # save results dictionary\n",
        "    with open('./logs/train_results_{}.pk'.format(train_config.model), 'wb') as file:\n",
        "        pickle.dump(train_results_folds, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "    tb_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------[ Training: inception_v3 ]---------------------------\n",
            "Batch Size: 15\n",
            "Epochs: 20\n",
            "Fold: 0 (1/5)\n",
            "-------------------------------[ Initial Fold: 0 ]------------------------------\n",
            "Validation loss: 2.5258, Accuracy: 88/1308 (6.728%)\n",
            "Initial Validation Loss : 2.525790, \n",
            "Initial Validation Accuracy : 6.728%\n",
            "----------------------------------[ Epoch: 0 ]----------------------------------\n",
            "Train Loss: 2.339771 Accuracy: 0.2607\n",
            "Elapsed 00:02:15, 00:02:15 time/epoch, 0.39 s/batch, remaining 00:42:53\n",
            "Validation loss: 1.8778, Accuracy: 557/1308 (42.584%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 1 ]----------------------------------\n",
            "Train Loss: 2.015054 Accuracy: 0.3927\n",
            "Elapsed 00:04:59, 00:02:29 time/epoch, 0.43 s/batch, remaining 00:44:55\n",
            "Validation loss: 1.5363, Accuracy: 635/1308 (48.547%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 2 ]----------------------------------\n",
            "Train Loss: 1.909825 Accuracy: 0.4220\n",
            "Elapsed 00:07:44, 00:02:34 time/epoch, 0.44 s/batch, remaining 00:43:49\n",
            "Validation loss: 1.4083, Accuracy: 669/1308 (51.147%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 3 ]----------------------------------\n",
            "Train Loss: 1.848476 Accuracy: 0.4313\n",
            "Elapsed 00:10:29, 00:02:37 time/epoch, 0.45 s/batch, remaining 00:41:56\n",
            "Validation loss: 1.1360, Accuracy: 715/1308 (54.664%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 4 ]----------------------------------\n",
            "Train Loss: 1.794851 Accuracy: 0.4521\n",
            "Elapsed 00:13:14, 00:02:38 time/epoch, 0.46 s/batch, remaining 00:39:42\n",
            "Validation loss: 1.2175, Accuracy: 645/1308 (49.312%)\n",
            "----------------------------------[ Epoch: 5 ]----------------------------------\n",
            "Train Loss: 1.764276 Accuracy: 0.4564\n",
            "Elapsed 00:15:57, 00:02:39 time/epoch, 0.46 s/batch, remaining 00:37:14\n",
            "Validation loss: 1.2973, Accuracy: 691/1308 (52.829%)\n",
            "----------------------------------[ Epoch: 6 ]----------------------------------\n",
            "Train Loss: 1.726947 Accuracy: 0.4722\n",
            "Elapsed 00:18:42, 00:02:40 time/epoch, 0.46 s/batch, remaining 00:34:44\n",
            "Validation loss: 1.1993, Accuracy: 689/1308 (52.676%)\n",
            "----------------------------------[ Epoch: 7 ]----------------------------------\n",
            "Train Loss: 1.742710 Accuracy: 0.4572\n",
            "Elapsed 00:21:26, 00:02:40 time/epoch, 0.46 s/batch, remaining 00:32:09\n",
            "Validation loss: 1.1445, Accuracy: 707/1308 (54.052%)\n",
            "----------------------------------[ Epoch: 8 ]----------------------------------\n",
            "Train Loss: 1.724306 Accuracy: 0.4659\n",
            "Elapsed 00:24:10, 00:02:41 time/epoch, 0.46 s/batch, remaining 00:29:33\n",
            "Validation loss: 1.1248, Accuracy: 736/1308 (56.269%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 9 ]----------------------------------\n",
            "Train Loss: 1.687358 Accuracy: 0.4736\n",
            "Elapsed 00:26:56, 00:02:41 time/epoch, 0.46 s/batch, remaining 00:26:56\n",
            "Validation loss: 1.3419, Accuracy: 690/1308 (52.752%)\n",
            "----------------------------------[ Epoch: 10 ]---------------------------------\n",
            "Train Loss: 1.690525 Accuracy: 0.4750\n",
            "Elapsed 00:29:40, 00:02:41 time/epoch, 0.46 s/batch, remaining 00:24:16\n",
            "Validation loss: 1.2566, Accuracy: 731/1308 (55.887%)\n",
            "----------------------------------[ Epoch: 11 ]---------------------------------\n",
            "Train Loss: 1.673886 Accuracy: 0.4842\n",
            "Elapsed 00:32:23, 00:02:41 time/epoch, 0.46 s/batch, remaining 00:21:35\n",
            "Validation loss: 1.3013, Accuracy: 716/1308 (54.740%)\n",
            "----------------------------------[ Epoch: 12 ]---------------------------------\n",
            "Train Loss: 1.680969 Accuracy: 0.4723\n",
            "Elapsed 00:35:06, 00:02:42 time/epoch, 0.46 s/batch, remaining 00:18:54\n",
            "Validation loss: 1.2909, Accuracy: 688/1308 (52.599%)\n",
            "----------------------------------[ Epoch: 13 ]---------------------------------\n",
            "Train Loss: 1.668613 Accuracy: 0.4798\n",
            "Elapsed 00:37:49, 00:02:42 time/epoch, 0.46 s/batch, remaining 00:16:12\n",
            "Validation loss: 1.2381, Accuracy: 698/1308 (53.364%)\n",
            "----------------------------------[ Epoch: 14 ]---------------------------------\n",
            "Train Loss: 1.664883 Accuracy: 0.4737\n",
            "Elapsed 00:40:31, 00:02:42 time/epoch, 0.46 s/batch, remaining 00:13:30\n",
            "Validation loss: 1.4614, Accuracy: 709/1308 (54.205%)\n",
            "----------------------------------[ Epoch: 15 ]---------------------------------\n",
            "Train Loss: 1.652043 Accuracy: 0.4873\n",
            "Elapsed 00:43:16, 00:02:42 time/epoch, 0.47 s/batch, remaining 00:10:49\n",
            "Validation loss: 1.2223, Accuracy: 703/1308 (53.746%)\n",
            "----------------------------------[ Epoch: 16 ]---------------------------------\n",
            "Train Loss: 1.655486 Accuracy: 0.4834\n",
            "Elapsed 00:46:03, 00:02:42 time/epoch, 0.47 s/batch, remaining 00:08:07\n",
            "Validation loss: 1.4148, Accuracy: 680/1308 (51.988%)\n",
            "----------------------------------[ Epoch: 17 ]---------------------------------\n",
            "Train Loss: 1.641111 Accuracy: 0.4781\n",
            "Elapsed 00:48:51, 00:02:42 time/epoch, 0.47 s/batch, remaining 00:05:25\n",
            "Validation loss: 1.3987, Accuracy: 710/1308 (54.281%)\n",
            "----------------------------------[ Epoch: 18 ]---------------------------------\n",
            "Train Loss: 1.638853 Accuracy: 0.4795\n",
            "Elapsed 00:51:32, 00:02:42 time/epoch, 0.47 s/batch, remaining 00:02:42\n",
            "Validation loss: 1.2818, Accuracy: 714/1308 (54.587%)\n",
            "----------------------------------[ Epoch: 19 ]---------------------------------\n",
            "Train Loss: 1.648019 Accuracy: 0.4885\n",
            "Elapsed 00:54:14, 00:02:42 time/epoch, 0.47 s/batch, remaining 00:00:00\n",
            "Validation loss: 1.4461, Accuracy: 720/1308 (55.046%)\n",
            "-------------------------------[ Result Fold: 0 ]-------------------------------\n",
            "Total time: 00:54:43 - Accuracy: 56.269% - Best Epoch: 8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       bhaji      0.481     0.405     0.440       126\n",
            "     chapati      0.627     0.815     0.709       173\n",
            "     githeri      0.562     0.760     0.646        96\n",
            "  kachumbari      0.386     0.323     0.352        99\n",
            "   kukuchoma      0.172     0.143     0.156        35\n",
            "     mandazi      0.852     0.742     0.793       124\n",
            " masalachips      0.596     0.644     0.619        87\n",
            "      matoke      0.510     0.552     0.530        96\n",
            "      mukimo      0.429     0.279     0.338        43\n",
            "  nyamachoma      0.696     0.603     0.646       156\n",
            "       pilau      0.650     0.591     0.619        66\n",
            "  sukumawiki      0.349     0.370     0.359        81\n",
            "       ugali      0.483     0.460     0.472       126\n",
            "\n",
            "    accuracy                          0.563      1308\n",
            "   macro avg      0.522     0.514     0.514      1308\n",
            "weighted avg      0.560     0.563     0.557      1308\n",
            "\n",
            "---------------------------------[ End Fold: 0 ]--------------------------------\n",
            "\n",
            "---------------------------[ Training: inception_v3 ]---------------------------\n",
            "Batch Size: 15\n",
            "Epochs: 20\n",
            "Fold: 1 (2/5)\n",
            "-------------------------------[ Initial Fold: 1 ]------------------------------\n",
            "Validation loss: 2.7342, Accuracy: 143/1307 (10.941%)\n",
            "Initial Validation Loss : 2.734196, \n",
            "Initial Validation Accuracy : 10.941%\n",
            "----------------------------------[ Epoch: 0 ]----------------------------------\n",
            "Train Loss: 2.357770 Accuracy: 0.2602\n",
            "Elapsed 00:02:14, 00:02:14 time/epoch, 0.39 s/batch, remaining 00:42:34\n",
            "Validation loss: 1.8046, Accuracy: 602/1307 (46.060%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 1 ]----------------------------------\n",
            "Train Loss: 2.023016 Accuracy: 0.4060\n",
            "Elapsed 00:04:56, 00:02:28 time/epoch, 0.42 s/batch, remaining 00:44:29\n",
            "Validation loss: 1.7712, Accuracy: 692/1307 (52.946%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 2 ]----------------------------------\n",
            "Train Loss: 1.910250 Accuracy: 0.4334\n",
            "Elapsed 00:07:38, 00:02:32 time/epoch, 0.44 s/batch, remaining 00:43:19\n",
            "Validation loss: 1.5277, Accuracy: 702/1307 (53.711%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 3 ]----------------------------------\n",
            "Train Loss: 1.860664 Accuracy: 0.4445\n",
            "Elapsed 00:10:26, 00:02:36 time/epoch, 0.45 s/batch, remaining 00:41:44\n",
            "Validation loss: 1.6855, Accuracy: 709/1307 (54.246%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 4 ]----------------------------------\n",
            "Train Loss: 1.820159 Accuracy: 0.4514\n",
            "Elapsed 00:13:12, 00:02:38 time/epoch, 0.45 s/batch, remaining 00:39:37\n",
            "Validation loss: 1.8103, Accuracy: 738/1307 (56.465%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 5 ]----------------------------------\n",
            "Train Loss: 1.767179 Accuracy: 0.4626\n",
            "Elapsed 00:15:55, 00:02:39 time/epoch, 0.46 s/batch, remaining 00:37:09\n",
            "Validation loss: 1.5169, Accuracy: 728/1307 (55.700%)\n",
            "----------------------------------[ Epoch: 6 ]----------------------------------\n",
            "Train Loss: 1.760137 Accuracy: 0.4620\n",
            "Elapsed 00:18:39, 00:02:39 time/epoch, 0.46 s/batch, remaining 00:34:39\n",
            "Validation loss: 1.5672, Accuracy: 737/1307 (56.389%)\n",
            "----------------------------------[ Epoch: 7 ]----------------------------------\n",
            "Train Loss: 1.741333 Accuracy: 0.4655\n",
            "Elapsed 00:21:23, 00:02:40 time/epoch, 0.46 s/batch, remaining 00:32:05\n",
            "Validation loss: 1.3196, Accuracy: 752/1307 (57.536%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 8 ]----------------------------------\n",
            "Train Loss: 1.732343 Accuracy: 0.4685\n",
            "Elapsed 00:24:07, 00:02:40 time/epoch, 0.46 s/batch, remaining 00:29:28\n",
            "Validation loss: 1.3692, Accuracy: 737/1307 (56.389%)\n",
            "----------------------------------[ Epoch: 9 ]----------------------------------\n",
            "Train Loss: 1.723685 Accuracy: 0.4768\n",
            "Elapsed 00:26:44, 00:02:40 time/epoch, 0.46 s/batch, remaining 00:26:44\n",
            "Validation loss: 1.2796, Accuracy: 734/1307 (56.159%)\n",
            "----------------------------------[ Epoch: 10 ]---------------------------------\n",
            "Train Loss: 1.719962 Accuracy: 0.4722\n",
            "Elapsed 00:29:22, 00:02:40 time/epoch, 0.46 s/batch, remaining 00:24:02\n",
            "Validation loss: 1.3959, Accuracy: 743/1307 (56.848%)\n",
            "----------------------------------[ Epoch: 11 ]---------------------------------\n",
            "Train Loss: 1.710704 Accuracy: 0.4732\n",
            "Elapsed 00:32:00, 00:02:40 time/epoch, 0.46 s/batch, remaining 00:21:20\n",
            "Validation loss: 1.5465, Accuracy: 741/1307 (56.695%)\n",
            "----------------------------------[ Epoch: 12 ]---------------------------------\n",
            "Train Loss: 1.713098 Accuracy: 0.4709\n",
            "Elapsed 00:34:37, 00:02:39 time/epoch, 0.46 s/batch, remaining 00:18:38\n",
            "Validation loss: 1.4069, Accuracy: 735/1307 (56.236%)\n",
            "----------------------------------[ Epoch: 13 ]---------------------------------\n",
            "Train Loss: 1.694791 Accuracy: 0.4804\n",
            "Elapsed 00:37:15, 00:02:39 time/epoch, 0.46 s/batch, remaining 00:15:58\n",
            "Validation loss: 1.7698, Accuracy: 732/1307 (56.006%)\n",
            "----------------------------------[ Epoch: 14 ]---------------------------------\n",
            "Train Loss: 1.687376 Accuracy: 0.4771\n",
            "Elapsed 00:39:53, 00:02:39 time/epoch, 0.46 s/batch, remaining 00:13:17\n",
            "Validation loss: 1.5265, Accuracy: 756/1307 (57.842%)\n",
            "Model Improved. Saving the Model.\n",
            "----------------------------------[ Epoch: 15 ]---------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtlgoUtBVk9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZf5GhAUfzS-",
        "colab_type": "text"
      },
      "source": [
        "#### Cross-Validation Result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDxjQBrCfzTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate Crossvalidation Result\n",
        "best_acc_folds = np.zeros(train_config.folds, dtype=np.float32)  \n",
        "\n",
        "print_seperator(80, '-', name='Cross-Validation Accuracy')\n",
        "\n",
        "for i in range(train_config.folds):\n",
        "    acc_fold_epochs = train_results_folds[i][\"val_acc\"]\n",
        "    \n",
        "    best_acc_fold = acc_fold_epochs.max()\n",
        "    best_acc_epoch = np.argmax(acc_fold_epochs)\n",
        "    \n",
        "    print(\"Fold_{}: {:.3f} (Epoch:{})\".format(i, best_acc_fold, best_acc_epoch))\n",
        "    \n",
        "    best_acc_folds[i]= best_acc_fold\n",
        "    \n",
        "crossvalidation_accuracy = best_acc_folds.mean()\n",
        "\n",
        "\n",
        "print(\"\\nCross-Validation Accuracy: {:.3f}%\".format(crossvalidation_accuracy*100))\n",
        "print_seperator(80, '-', name='End Cross-Validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSPO7EdUfzTF",
        "colab_type": "text"
      },
      "source": [
        "### <font style=\"color:green\">7.1 Predict and submission.csv</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV5c49fVfzTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probs_folds = []\n",
        "\n",
        "if train_config.model == 'resnext50_32x4d':\n",
        "    model = resnext50_32x4d(transfer_learning=True, num_class=13)\n",
        "else:\n",
        "    model = resnet50(transfer_learning=True, num_class=13)\n",
        "    \n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for fold in range(0, train_config.folds):\n",
        "    print(\"Predict Fold: {}\".format(fold))\n",
        "    train_config.fold = fold\n",
        "    train_config.save_name = 'fold_{}_{}.pt'.format(fold, train_config.model)\n",
        "    \n",
        "    model = load_model(model, model_dir='models', model_file_name=train_config.save_name)\n",
        "    \n",
        "    # data loader\n",
        "    _, _, test_loader = get_data(batch_size=train_config.batch_size,\n",
        "                                 data_root=train_config.data_root,\n",
        "                                 num_workers=train_config.num_workers,\n",
        "                                 folds=train_config.folds,\n",
        "                                 fold=train_config.fold)\n",
        "    \n",
        "    probs = predict(train_config, model, test_loader)\n",
        "    \n",
        "    probs_folds.append(probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f71Q-nf_fzTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sum probs over all folds\n",
        "probs = np.array(probs_folds).sum(0)\n",
        "predictions = np.argmax(probs,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBdhxMVsfzTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create submmison dataframe\n",
        "submission_df = test_loader.dataset.data_df\n",
        "submission_df[\"class\"] = predictions\n",
        "\n",
        "submission_df[\"class\"] = submission_df[\"class\"].map(test_loader.dataset.id_to_label)\n",
        "\n",
        "submission_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEEF2yGYfzTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save submission dataframe\n",
        "submission_path = os.path.join(train_config.data_root, 'submission.csv')\n",
        "submission_df.to_csv(submission_path, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq0krCIkfzTW",
        "colab_type": "text"
      },
      "source": [
        "## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n",
        "\n",
        "Share your tensorboard scalars logs link in this section. You can also share (not mandatory) your GitHub link if you have pushed this project in GitHub. \n",
        "\n",
        "For example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn3CVt6zfzTW",
        "colab_type": "text"
      },
      "source": [
        "Link: https://tensorboard.dev/experiment/iDofl6ypRDS3wdCZBrlkQQ/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIYamJFsfzTX",
        "colab_type": "text"
      },
      "source": [
        "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
        "\n",
        "Share your Kaggle profile link here with us so that we can give points for the competition score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsiiQ_KpfzTX",
        "colab_type": "text"
      },
      "source": [
        "Kaggle Link: https://www.kaggle.com/khabel"
      ]
    }
  ]
}